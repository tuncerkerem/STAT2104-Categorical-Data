---
title: 'Applied Categorical Data: Homework 3'
author: "Kerem Tuncer"
date: "2/19/2021"
output:
  word_document: default
  html_notebook: default
---

## Question 1

Part A.

```{r}
rm(list = ls())
dat <- read.table("http://users.stat.ufl.edu/~aa/cat/data/Crabs.dat", 
           header=TRUE)
```

```{r}
model <- lm(y ~ weight, data = dat)
summary(model)
```

The equation is $\pi(x) = -0.14487 + 0.32270*weight$. Here the Beta coefficient is 0.32270. Given that this is a linear probability model, this coefficient implies that a change in the crab weight by 1 is estimated to increase the probability that crab has one or more satellites by 0.32270.


Part B.

Let's do $x = 4$ and $x = 5$.

```{r}
-0.14487 + 0.32270*4
-0.14487 + 0.32270*5
```

For x = 4, we get $\hat{P}(Y = 1) = 1.1459$ and for x = 5, we get $\hat{P}(Y = 1) = 1.46863$. Both of these values are much higher than the upper bound of 1.0 for a probability, which is not realistic as the probability has to be in the range [0,1],  This is a known disadvantage of a linear probability model, where linear predictors take values over the entire real line.

Part C. 

```{r}
mylogit <- glm(y ~ weight, data = dat, family = "binomial")
summary(mylogit)
exp(coef(mylogit))
```

The logistic regression model is $log[\,\frac{\pi(x)}{1-\pi(x)}]\,= -3.6947 + 1.8151*weight$. Since the Beta coefficient > 0, the estimated probability of having one or more than one satellite increases as the weight of the crab increases. 

Part D.

Let's do $x = 4$ and $x = 5$.

```{r}
-3.6947 + 1.8151*4
-3.6947 + 1.8151*5
```

These are our estimate of $log[\,\frac{\pi(x)}{1-\pi(x)}]\,$. Now, let's find their values of $\hat\pi$.

```{r}
pi_for_x4 = exp(3.5657) / (1 + exp(3.5657))
pi_for_x5 = exp(5.3808) / (1 + exp(5.3808))
pi_for_x4
pi_for_x5
```

The log odds for success (meaning that a crab has at least one satellite) for x = 4 was 3.5657. This translates to a probability of for success (meaning that a crab has at least one satellite) of 0.9725004. The log odds for success (meaning that a crab has at least one satellite) for x = 5 was 5.3808. This translates to a probability of for success (meaning that a crab has at least one satellite) of 0.995417. Given these probabilities, it is likely that crabs with these weights will have at least one satellite.

## Question 2

Part A.

```{r}
credit <- read.table("http://users.stat.ufl.edu/~aa/cat/data/Credit.dat", 
           header=TRUE)
credat <- read.csv("credit.csv", fileEncoding = "UTF-8-BOM")
```


```{r}
creditmodel <- glm(cards~ income, data = credat, family = "binomial")

summary(creditmodel)
```

After manually transforming the data, I have run the logistic regression and got the following prediction equation:

$$log[\,\frac{\pi(x)}{1-\pi(x)}]\,= -3.51795 + 0.10541*income$$
The sign of the Beta coefficient is positive given that 0.10541 > 0. This means that the estimated probability of having one or more credit cards increases as the annual income increases. 

Part B.

$$log[\,\frac{0.5}{1-0.5}]\,= log[\,\frac{0.5}{0.5}]\, =-3.51795 + 0.10541*income$$

$$log[\,1]\,= -3.51795 + 0.10541*income$$
$$ 0 = 3.51795 - 0.10541 * income $$

$$income = \frac{3.51795}{0.10541}$$


```{r}
income = 3.51795/0.10541
income
```

Therefore, at income =  33.37397, the probability of a travel credit card is 0.5.

## Question 3

Part A.

```{r}
poisson.mod <- glm(sat ~ weight, family="poisson", data=dat)
summary(poisson.mod)
```


This the prediction equation:

$$log(\hat{\mu}) = - 0.42841 + 0.58930*weight$$

Now, let's predict the mean response for female crabs of average weight 2.44 kg.

```{r}
log_mu <- -0.42841 + 0.58930*2.44
exp(log_mu)
```

The estimated mean response for female crabs with average weight of 2.44 kg is 2.744179.

Part B.

```{r}
library(aod)
library(lmtest)
library(DescTools)
```

Let's first state the hypotheses.

H0: The mean of Y is independent of weight, which means that Beta = 0.

Ha: THe mean of Y is not independent of weight, which means that Beta != 0.

```{r}
wald.test(b=coef(poisson.mod), Sigma=vcov(poisson.mod), Terms = 2)
lrtest(poisson.mod)
```

The X2 for the Wald test is 82.2 and the LR statistic is 71.925. In both cases df is equal to 1. Both of the tests have a p-values that is much below 0.05 and is statistically significant. There is strong evidence to suggest that the mean response is not independent of weight. All in all, there is strong evidence of some effect of weight on the mean response variable and that Beta != 0..


Part C.

Given that exp(beta) represents the multiplicative effect on the fitted value for each unit of increase in weight, 1-kg increase in weight has results in an increase in the estimated mean number of satellites by a factor of 1.8 (that is exp(0.5893)).

Here, we have to use the confint.default function instead of confint so we can get the Wald confidence intervals.

```{r}
confint.default(poisson.mod, level = 0.95)
```

The 95% confidence interval is [0.4618742, 0.716734]. So, we are 95% confident that the true B for weight will lie in that interval.

To calculate the 95% confidence interval for the multiplicative effect of a 1-kg increase, we will exponentiate the confidence interval for Beta.

```{r}
conf <- c(exp(0.4618742),exp(0.716734))
conf
```


The 95% CI for multiplicative effect of a 1 kg increase is [1.587046, 2.047734]. So we are 95% confident that the true multiplicative effect of a 1 kg increase is between 1.587046 and 2.047734.

## Question 4

Part A.

First, I will start by proving that $e^{\beta}=\mu_B/\mu_A$

$$x = 0 \: for \: treatment \:A$$

So,

$$log(\mu_A)=\alpha + \beta *0 = \alpha$$


Then, 

$$x = 1 \: for \: treatment \:B$$


So,

$$log(\mu_B)=\alpha + \beta *1 = \alpha + \beta$$

By subtraction, this means that,

$$\beta = log(\mu_B) - log(\mu_A) = log(\mu_B/\mu_A)$$

Lastly, we will exponentiate

$$e^{\beta}=\mu_B/\mu_A$$

Now, I will prove that $e^{\alpha}=\mu_{A}$.

As I previously mentioned,

$$x = 0 \: for \: treatment \:A$$

So,

$$log(\mu_A)=\alpha + \beta *0 = \alpha$$

If $log(\mu_A)=\alpha$, then we can exponentiate to get,

$$\mu_{A} = e^{\alpha}$$

Part B.

```{r}
imperfections <- c(8, 7, 6, 6, 3, 4, 7, 2, 3, 4, 9, 9, 8, 14,
8, 13, 11, 5, 7, 6)

treatment <- c(rep(0,10), rep(1,10))

df <- data.frame(imperfections, treatment)
```

```{r}
final.mod <- glm(imperfections ~ treatment, family="poisson", data=df)
summary(final.mod)
```


Accordingly, the prediction equation is $log(\hat\mu) = 1.6094 + 0.5878x$. Let's first interpret the beta coefficient. Given that $log(\hat\mu_B/\hat\mu_A) = 0.5878, \hat\mu_B = e^{0.5878}\hat\mu_A \approx1.8\hat\mu_A$. This means that the average number of imperfections using method B is about 1.8 times that of using treatment A. Given that x = 0, refers to treatment A, the alpha value (which is the intercept) of 1.6094 implies that $log(\mu_A) = 1.6094$. After exponentiating, this becomes $\mu_A= e^{1.6094} = 4.99981.$  As a result, the intercept of 4.99981 means that treatment A is has a mean imperfection amount of 4.99981. Likewise, our beta value means that the estimated mean of B is exp(0.5878) (which is 1.8), greater than estimated mean of A.

Part C. 

```{r}
wald.test(b=coef(final.mod), Sigma=vcov(final.mod), Terms = 2)
```

We have a null hypothesis that mean of A is equal to mean of B. Then, we have an alternative hypothesis that the mean of A is not equal to mean of B.

Given that the p-values is way below 0.05, we can reject the null hypothesis. We have enough evidence to conclude that there is a difference between the means of the two groups. 


Part D.

```{r}
confint.default(final.mod, level = 0.95)
```

```{r}
expo <- c(exp(0.242082),exp(0.9334913))
expo
```

Our 95% confidence interval is [1.273899, 2.543373]. With 95% certainty, we can say that the estimated mean for treatment B is 1.273899 times to 2.543373 times higher than the estimated mean of treatment A. This finding agrees with our earlier hypothesis test that concluded that there was enough evidence for significant difference in means and also agrees with the sign of our coefficient estimate. FYI, I used the Wald confidence intervals once again.









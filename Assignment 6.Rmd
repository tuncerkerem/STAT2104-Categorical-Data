---
title: 'Applied Categorical Data Analysis: Homework 6'
author: "Kerem Tuncer"
date: "04/10/2021"
output:
  word_document: default
  html_notebook: default
---

## Question 1

```{r}
rm(list = ls())
```

### Part A

We should keep in mind that a positive slope in the cumulative logit model (with the proportional odds property) indicates a negative association between x and the ordinal variable Y, whereas a negative slope indicates a positive association between x and the ordinal variable Y.

For $x_1$, we have $\beta < 0$. Hence, the response level increases with an increase in $x_1$. This means that, job satisfaction tends to increase at higher x1, which is earnings compared to others with similar positions.

For $x_2$, we have $\beta > 0$. Hence, the response level increases with a decrease in $x_2$. This means that, job satisfaction tends to increase at lower x2, which is freedom to make decisions.

For $x_3$, we have $\beta > 0$. Hence, the response level increases with a decrease in $x_3$. This means that, job satisfaction tends to increase at lower x3, which is work environment productivity.

These interpretations all make sense logically speaking.

### Part B

For $x_1$, we have $\beta < 0$. Hence, there is a positive association, meaning that Y will be the highest (which is 4) when x1 is the highest which is x1 = 4 (much more earnings compared to others with similar positions).

For $x_2$, we have $\beta > 0$. Hence, there is a negative association, meaning that Y will be the highest (which is 4) when x2 is the lowest which is x2 = 1 (very true about being free to make decisions).

For $x_3$, we have $\beta > 0$. Hence, there is a negative association, meaning that Y will be the highest (which is 4) when x3 is the lowest which is x3 = 1 (strongly agree about work environment allowing productivity).

### Part C

Given that $logit[\hat P >=j]$ is the reciprocal of $logit[\hat P <= j]$, we can say that the signs of the slopes for x1, x2, and x3 will change. The slope of x1 will become positive, and the slopes of x2 and x3 will become negative.

## Question 2

### Part A

```{r}
library(VGAM)
Not <- c(6,6,6); 
Pretty <- c(43,113,57); 
Very <- c(75,178,117);
Income <- c("Below", "Average", "Above")
data.frame(Income, Not, Pretty, Very)
scores<-c(1,2,3)
logitmodel <- vglm(cbind(Not,Pretty,Very) ~ scores, family=cumulative(parallel=TRUE))
summary(logitmodel)
```

The prediction equations are the following:

For j = 1,

$$logit[P(\hat Y ≤ 1)] = -3.2466 - 0.1117*score $$

For j = 2,

$$logit[P(\hat Y ≤ 2)] = -0.2378 - 0.1117*score $$

### Part B

```{r}
exp(-coef(logitmodel)[3])
```

For any fixed j, a unit increase in income score (from a lower score group to a higher score group) will increase the odds of being happier by a multiplicative effect of exp(0.1117423)=1.118225.

### Part C

Null hypothesis: Beta = 0, meaning that there is no effect of income on marital happiness. In other words, the two things are independent.

Alternative hypothesis: Beta ! 0, meaning that there is an effect of income on marital happiness. In other words, the two things are not independent.

```{r}
logit.null <- update(logitmodel, ~ 1)
lrtest(logitmodel, logit.null)
```

Our test statistic is equal to 0.8876 and p-value of 0.3461. Given that the p-value is much larger than 0.05, we do not have enough evidence to conclude that income has an effect on marital happiness. This is in line with our findings from the previous homework.

### Part D

```{r}
summary(logitmodel)
```

The residual deviance is 3.2472 and the degrees of freedom is 3. With this information, we can compute a deviance goodness-of-fit test.

```{r}
1-pchisq(3.2472, 3)
```
H0: model fits adequately

Ha: model does not fit adequately

As a result, we get the p-value of 0.3550593. Given that the p-value is above 0.05, we can say that the model fits adequately.

### Part E

```{r}
predict(logitmodel, data.frame(scores = 2), type="response")
```

The probability that a person with average family income (score = 2) reports a very happy marriage is 0.6133082.

## Question 3

### Part A


```{r}
Heaven_Hell <- matrix(c(833,125,2,160),ncol=2,byrow=TRUE)
colnames(Heaven_Hell) <- c("Hell (Yes)","Hell (No)")
rownames(Heaven_Hell) <- c("Heaven (Yes)","Heaven (No)")
Heaven_Hell <- as.table(Heaven_Hell)
Heaven_Hell
```

$$H_0 : P(Y_1=1) = P(Y_2 = 1)$$
In words, that would be that the population proportions answering yes were the same for heaven and hell.

$$H_a :  P(Y_1=1) \neq P(Y_2 = 1)$$
In words, that would be that the population proportions answering yes were not the same for heaven and hell.

```{r}
mcnemar.test(Heaven_Hell, correct = F)
```

Our chi-squared statistic is 119.3 and df = 1, so our p-value is < 2.2e-. Given that this value is much below 0.05, we can reject the null hypothesis. We have enough to conclude that the population proportions answering yes were not the same for heaven and hell.

### Part B

```{r}
SE <- 1/1120 * sqrt( 125 + 2 - (125-2)^2 / 1120)
SE
CI <- ((125 - 2) / 1120) + c(-1,1) * 1.64 * SE
CI
```

The standard error was 0.00951184. Therefore, our 90% confidence interval is [0.09422201, 0.12542085]. We are 90% confident that the difference between the population proportions, for those who believe in Hell and those who believe in Heaven, lies between the range 0.09422201 and 0.12542085.


## Question 4

### Part A

```{r}
filename <- "http://users.stat.ufl.edu/~aa/cat/data/DeathPenalty.dat"
library(readr)
Data <- read.table(file=filename, header=T)
log.lin.mod <- glm(count ~ (D+V+P)^2, data=Data, family=poisson)
summary(log.lin.mod)
```
The residual deviance is 0.37984 and the degrees of freedom is 1. With this information, we can compute a deviance goodness-of-fit test.

```{r}
1-pchisq(0.37984, 1)
```

H0: model fits adequately

Ha: model does not fit adequately

We have a residual deviance of 0.37984 and the degrees of freedom is 1. As a result, we get the p-value of 0.5376889. Given that the p-value is above 0.05, we can say that the model fits adequately.

### Part B

```{r}
exp(-0.8678)
```
The estimated conditional odds ratio between D and P at each category of V is 0.4198743. If we were to keep the victims' race constant, then the estimated odds for a white defendant receiving the death penalty is 0.4198743 times the odds of a black defendant.

### Part C

```{r}
((53+0)/(414+16))/((11+4)/(139+37))
```
The marginal odds ratio between D and P is 1.446202, which means that the estimated odds of a white person receiving the death penalty is 1.446202 times the odds of a black person receiving the death penalty. Our conditional odds ratio was less than 1, but marginal odds ratio is greater than 1. This a case of the Simpson's paradox, where a trend appears in several different groups of data but disappears or reverses when these groups are combined. If we look at the data marginally, than the black defendants have less chance of death penalty. However, if we control for victim race, then the black defendants have higher chance of death penalty. In this specific case, the Simpson's Paradox occurs because white defendants had more white victims than black defendants, and most death penalty verdicts were given to cases with white victims. This suggests that the marginal association should show a greater tendency for white defendants to receive the death penalty than do the conditional associations, as there were much more cases with white defendants.

### Part D

```{r}
V <- c("white", "white", "black", "black")
D <- c("white", "black", "white", "black")
Yes <- c(53, 11, 0, 4)
No <- c(414, 37, 16, 139)
Data2 <- data.frame(D, V, Yes, No)
Data2
fit.logit <- glm(cbind(Yes,No) ~ D+V, data= Data2, family=binomial)
summary(fit.logit)
```

Here is the corresponding model:

$$logit(\hat\pi) = -3.5961 -0.8678*defendant + 2.4044*victim $$

As you can see, the coefficients for defendant(white) and victim(white) are the same as the coefficients for DWhite:Pyes and VWhite:PYes in the loglinear model, respectively.

## Question 5

### Part A

```{r}
mbti <- read.table("http://users.stat.ufl.edu/~aa/intro-cda/data/MBTI.dat", header = T)
```


```{r}
mbti.mod<- glm(n ~ EI + SN + TF + JP, family = poisson, data = mbti)
summary(mbti.mod)
### residual deviance G2
deviance(mbti.mod)
1 - pchisq(deviance(mbti.mod),df.residual(mbti.mod))

## Pearson chisquare
sum(residuals(mbti.mod, type = "pearson")^2)
1 - pchisq(sum(residuals(mbti.mod, type = "pearson")^2),df.residual(mbti.mod))
```

H0: Model fits adequately.
Ha: Model does not fit adequately.

The residual deviance is 135.8672 and the Pearson Chi-squared is 145.1028. In both cases, the p-value converges to zero. Given that the p-value is much below 0.05, we can reject the null hypothesis and say that the model fits poorly.

### Part B

```{r}
mbti.hmod <- glm(formula = n ~ (EI + SN + TF + JP)^2, family = poisson, data = mbti)
summary(mbti.hmod)
### residual deviance G2
deviance(mbti.hmod)
1 - pchisq(deviance(mbti.hmod),df.residual(mbti.hmod))

### Pearson chisquare
sum(residuals(mbti.hmod, type = "pearson")^2)
1 - pchisq(sum(residuals(mbti.hmod, type = "pearson")^2),df.residual(mbti.hmod))
```

H0: Model fits adequately.
Ha: Model does not fit adequately.

The residual deviance is 10.16171 and the Pearson Chi-squared is 10.10336. The residual deviance p-value is 0.0707809 and the pearson chi-squared p-value is 0.07235899. Hence, we cannot reject the null hypothesis. We have enough evidence to conclude that this model is adequate and has a much better fit than the previous one.


### Part i

The parameter estimate for the conditional log odds ratio between the S/N and J/P scales is -1.222, larger than any of the others in absolute value. Likewise, it has the smallest p-value among the conditional associations. Therefore, the estimated conditional association is strongest between the S/N and J/P scales.

### Part ii

```{r}
#wald chi square statistics
1.482 ^ 2
0.134 ^ 2
```

The corresponding Wald chi-squared statistic for the log odds ratio between the EI and TF scale is 2.20 (which is z-value^2 = 1.482^2). The corresponding Wald chi-squared statistic for the log odds ratio between the EI and JP scale is 0.017956 (which is z-value^2 = 0.134^2). They both have a degree of freedom equal to 1. According to this information, they have respectively the p-values of 0.138258 and 0.893261. Both of these p-values are greater than 0.05, which accordingly shows that the conditional association between the scales are insignificant.

### Part C

```{r}
mbti2.mod <- glm(n ~ EI + SN + TF + JP + EI:SN + SN:TF + SN:JP + TF:JP, family = poisson, data = mbti)
summary(mbti2.mod)
```
```{r}
anova(mbti2.mod, mbti.hmod, test = "Chisq")
```

The difference in deviances is 12.369 - 10.162 = 2.207, and the df is 7 - 5 = 2. Therefore, we get a p-value of 0.3317. Given that this p-value is much higher than 0.05, we can say that the simpler model that assumes conditional independence between E/I and T/F, and between E/I and J/P, has a better fit than the model of homogenous association.

```{r}
exp(confint(mbti2.mod, method="profile"))
```

The 95% likelihood-ratio confidence interval for the conditional odds ratio between the S/N and J/P scales is [0.2214587, 0.3913279]. We are 95% confident that the that the odds of being N given the odds of being J are between 0.2214587 and 0.3913279.

### Part D

```{r}
final.mod <- glm(formula = n ~ (EI + SN + TF + JP)^3, family = poisson, data = mbti)
length(coef(mbti.mod))
length(coef(mbti.hmod))
length(coef(final.mod))
summary(mbti.mod)
summary(mbti.hmod)
summary(final.mod)
```

The loglinear model of mutual independence will have a total of 5 parameters (1 intercept + 4 main effects).

The model of homogeneous association will have a total of 11 parameters (1 intercept + 4 main effects + 6 two-factor association terms).

The model containing all three-factor interaction terms will have a total of 15 parameters (1 intercept + 4 main effects + 6 two-factor association terms + 4 three-factor interaction terms).

The R output above confirms this, as well.
